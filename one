# 1. Create the Database
spark.sql("CREATE DATABASE IF NOT EXISTS test")

# 2. Create DataFrame to simulate data
data = [(1, 'Alice', 25), (2, 'Bob', 30), (3, 'Charlie', 35)]
columns = ['id', 'name', 'age']
df = spark.createDataFrame(data, columns)

# 3. Write DataFrame to a Delta Table inside the test database
df.write.format("delta").mode("overwrite").saveAsTable("test.dkp_test")

# 4. Read Data from the Delta Table inside the test database
delta_df = spark.sql("SELECT * FROM test.dkp_test")
delta_df.show()

# 5. Filter Data (e.g., find people with age > 28)
filtered_df = delta_df.filter(delta_df.age > 28)
filtered_df.show()

# 6. Aggregation on Data (e.g., average age by name)
agg_df = delta_df.groupBy("name").avg("age")
agg_df.show()

# 7. Write Aggregated Data back to Delta Table
agg_df.write.format("delta").mode("overwrite").saveAsTable("test.dkp_test")

# 8. Save Data to Hive Metastore (overwrite if table exists)
df.write.mode("overwrite").saveAsTable("test.hive_metastore_test_table")

# 9. Create Delta Table
delta_path = "/mnt/delta/delta_table_example"
df.write.format("delta").mode("overwrite").save(delta_path)
df_read = spark.read.format("delta").load(delta_path)
df_read.show()

# 10. Save Data to DBFS as CSV
df.write.csv("/dbfs/tmp/test_csv_data.csv")

# 11. Read CSV data from DBFS
df_read_csv = spark.read.csv("/dbfs/tmp/test_csv_data.csv")
df_read_csv.show()

# 12. Check Databricks Runtime Version
print(f"Databricks Runtime Version: {spark.version}")

