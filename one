from pyspark.sql import SparkSession

spark = SparkSession.builder.appName("DeltaJobSetup").getOrCreate()

delta_table_path = "/mnt/your-mount-point/delta-table"
table_name = "my_delta_table"

spark.sql(f"""
CREATE TABLE IF NOT EXISTS {table_name} (
    id INT,
    name STRING,
    age INT
) USING DELTA LOCATION '{delta_table_path}'
""")

data = [(1, 'Alice', 25), (2, 'Bob', 30), (3, 'Charlie', 35)]
columns = ['id', 'name', 'age']
df = spark.createDataFrame(data, columns)

df.write.format("delta").mode("overwrite").save(delta_table_path)

spark.sql(f"CREATE TABLE IF NOT EXISTS {table_name} USING DELTA LOCATION '{delta_table_path}'")

df.show()

spark.stop()




from pyspark.sql import SparkSession

spark = SparkSession.builder.appName("DeltaJob").getOrCreate()

delta_table_path = "/mnt/your-mount-point/delta-table"
table_name = "my_delta_table"

delta_df = spark.read.format("delta").load(delta_table_path)

delta_df.show()

filtered_df = delta_df.filter(delta_df.age > 28)
filtered_df.show()

agg_df = delta_df.groupBy("name").avg("age")
agg_df.show()

agg_df.write.format("delta").mode("overwrite").save(delta_table_path)

spark.stop()
